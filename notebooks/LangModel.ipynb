{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandr.khvorov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "import urllib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_words(text: str):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def tokenize_to_sents(text: str):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def read_nips(path: str, documents_limit=None) -> List[List[str]]:\n",
    "    df = pd.read_csv(path, compression='gzip', sep=',')\n",
    "    docs = df['paper_text'].values.astype(np.str)\n",
    "    sents = []\n",
    "    for doc in docs if documents_limit is None else docs[:documents_limit]:\n",
    "        sents += [tokenize_to_words(s) for s in tokenize_to_sents(doc)]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_nips(\"../resources/datasets/nips-papers.csv.gz\", documents_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boosting', 'Density', 'Estimation', 'Saharon', 'Rosset', 'Department', 'of', 'Statistics', 'Stanford', 'University', 'Stanford', ',', 'CA', ',', '94305', 'saharon', '@', 'stat.stanford.edu', 'Eran', 'Segal', 'Computer', 'Science', 'Department', 'Stanford', 'University', 'Stanford', ',', 'CA', ',', '94305', 'eran', '@', 'cs.stanford.edu', 'Abstract', 'Several', 'authors', 'have', 'suggested', 'viewing', 'boosting', 'as', 'a', 'gradient', 'descent', 'search', 'for', 'a', 'good', 'fit', 'in', 'function', 'space', '.']\n",
      "2995\n",
      "6032\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(len(data))\n",
    "vocabulary = set([w for s in data for w in s])\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove(directory):\n",
    "    testfile.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", directory)\n",
    "\n",
    "def glove_embs(glove_path: str, vocab_size=400000, dim=50):\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "    assert dim in {50, 100, 200, 300}\n",
    "    with open(f'{glove_path}/glove.6B.{dim}d.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vectors.append(vect)\n",
    "    vectors = np.array(vectors).reshape((vocab_size, dim))\n",
    "    return words, word2idx, vectors\n",
    "   \n",
    "dim = 50\n",
    "words, word2idx, vectors = glove_embs(\"../resources/models/glove.6B\", dim=dim)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"UNK\"\n",
    "vocab = vocabulary & set(words)\n",
    "for s in data:\n",
    "    for i in range(len(s)):\n",
    "        if s[i] not in vocab:\n",
    "            s[i] = UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_vec = np.mean(vectors, axis=0)\n",
    "vocab.add(UNK)\n",
    "words.append(UNK)\n",
    "word2idx[UNK] = len(vectors)\n",
    "vectors = np.vstack((vectors, unk_vec.reshape(1, dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, weights_matrix, context_size=2):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "        vocab_size = weights_matrix.shape[0]\n",
    "        embedding_dim = weights_matrix.shape[1]\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "CONTEXT_SIZE=2\n",
    "for sent in data:\n",
    "    for i in range(len(sent) - CONTEXT_SIZE):\n",
    "        ngrams.append(([sent[i + j] for j in range(CONTEXT_SIZE)], sent[i + CONTEXT_SIZE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(vectors, context_size=CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 73.68544435501099\n",
      "1 60.876033306121826\n",
      "2 49.5262188911438\n",
      "3 37.369999408721924\n",
      "4 27.38015604019165\n",
      "5 20.479694843292236\n",
      "6 15.609149932861328\n",
      "7 11.868441581726074\n",
      "8 9.150538444519043\n",
      "9 7.463377952575684\n",
      "10 6.3802995681762695\n",
      "11 5.542942047119141\n",
      "12 4.854039192199707\n",
      "13 4.271095275878906\n",
      "14 3.7719554901123047\n",
      "15 3.347484588623047\n",
      "16 2.9810791015625\n",
      "17 2.6672191619873047\n",
      "18 2.39827823638916\n",
      "19 2.1705379486083984\n",
      "[128.66961288452148, 126.67717933654785, 124.57047176361084, 122.18837261199951, 119.34571743011475, 115.8079195022583, 111.27924823760986, 105.3598518371582, 97.52157735824585, 87.12769889831543, 73.68544435501099, 60.876033306121826, 49.5262188911438, 37.369999408721924, 27.38015604019165, 20.479694843292236, 15.609149932861328, 11.868441581726074, 9.150538444519043, 7.463377952575684, 6.3802995681762695, 5.542942047119141, 4.854039192199707, 4.271095275878906, 3.7719554901123047, 3.347484588623047, 2.9810791015625, 2.6672191619873047, 2.39827823638916, 2.1705379486083984]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams[35:45]:\n",
    "        context_idxs = torch.tensor([word2idx[w] for w in context], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.tensor([word2idx[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    print(epoch, total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['authors', 'have'], 'suggested'),\n",
       " (['have', 'suggested'], 'viewing'),\n",
       " (['suggested', 'viewing'], 'boosting'),\n",
       " (['viewing', 'boosting'], 'as'),\n",
       " (['boosting', 'as'], 'a'),\n",
       " (['as', 'a'], 'gradient'),\n",
       " (['a', 'gradient'], 'descent'),\n",
       " (['gradient', 'descent'], 'search'),\n",
       " (['descent', 'search'], 'for'),\n",
       " (['search', 'for'], 'a')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams[35:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7130\n",
      "7130\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor([word2idx[w] for w in ['suggested', 'viewing']], dtype=torch.long)).argmax().item())\n",
    "print(word2idx['boosting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN22(nn.Module):\n",
    "    def __init__(self, dim=50):\n",
    "        super(FCNN22, self).__init__()\n",
    "        self.fc1 = nn.Linear(4 * dim, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
